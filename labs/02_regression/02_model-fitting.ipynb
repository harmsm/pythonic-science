{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import scipy.optimize, scipy.stats\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter uncertainty\n",
    "\n",
    "#### Just how confident are you in your parameter values, anyway?\n",
    "\n",
    "You measure an observable over time and would like to extract a rate constant $k$ for the process.  You are fitting the data using the code below. Each data point was measured independently of the others.  The uncertainty in each measured point is normally distributed with a standard deviation of 0.05. \n",
    "\n",
    "*How robust are your estimates of $A$ and $k$ given the uncertainty in your measured points?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def first_order(t,A,k):\n",
    "    \"\"\"\n",
    "    First-order kinetics model. \n",
    "    \"\"\"\n",
    "    \n",
    "    return A*(1 - np.exp(-k*t))\n",
    "\n",
    "def first_order_r(param,t,obs):\n",
    "    \"\"\"\n",
    "    Residuals function for first-order model.\n",
    "    \"\"\"\n",
    "    return first_order(t,param[0],param[1]) - obs\n",
    "\n",
    "def fit_model(t,obs,param_guesses=(1,1)):\n",
    "    \"\"\"\n",
    "    Fit the first-order model.\n",
    "    \"\"\"\n",
    "\n",
    "    fit = scipy.optimize.least_squares(first_order_r,\n",
    "                                       param_guesses,\n",
    "                                       args=(t,obs))\n",
    "    fit_A = fit.x[0]\n",
    "    fit_k = fit.x[1]\n",
    "\n",
    "    return fit_A, fit_k\n",
    "\n",
    "d = pd.read_csv(\"data/time_course_0.csv\")\n",
    "A, k = fit_model(d.t,d.obs)\n",
    "plt.plot(d.t,d.obs,'o')\n",
    "plt.plot(d.t,first_order(d.t,A,k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Generate 1,000 simulated data sets where each experimental point is drawn from a normal distribution with a mean of d.obs and a standard deviation of 0.05.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Generate a histogram of possible values of $A$ and $k$ from these simulations.  (Hint: try fitting each simulated dataset independently...).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ What are the 95% confidence intervals on your estimate of $k$?  The lower bound is the value of $k$ for which 2.5% of the histogram counts are below the value.  The upper bound is the value of $k$ for which 2.5% of the histogram counts are above the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ You measure the same process under slightly different conditions.  These data are stored in `data/time_course_1.csv`.  Is there a statistically significant difference between $k$ from dataset 1 vs. 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: \n",
    "These are a couple of challenge questions for students who are already comfortable fitting and want to extend this basic fitting example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ You repeat the experiment again and explicitly measure the variance of each point (meaning some are now 0.1, others 0.3, etc.).  These values are stored in `data/time_course_2.csv` in the `obs_err` column.  Modify your sampling code so you incorporate these point-specific uncertainties. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ \"Global fitting\" is a powerful way to squeeze the most information out of experiments.  Rather than fitting models to individual experimental replicates and then estimating parameter uncertainty, one can fit all experiments at once to extract a single estimate of the fit parameters.  Implement a global fit function that simultaneously fits the data in `data/time_course_2.csv`, `data/time_course_3.csv`, and `data/time_course_4.csv`, then report your estimate and 95% confidence intervals of $k$. Do your confidence intervals change with more data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "\n",
    "#### How do you choose which model to use? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Akaike Information Criterion (AIC) helps you select between competing models.  It penalizes models for the number of fittable parameters because adding parameters will almost always make a fit better.   \n",
    "\n",
    "$$AIC = -2ln(\\hat{L}) + 2k$$\n",
    "\n",
    "where $\\hat{L}$ is the \"maximum likelihood\" and $k$ is (1 + number of fittable parameters in the model).  $\\hat{L}$ is proportional to the sum of the squared residuals ($SSR$), so we can write:\n",
    "\n",
    "$$ AIC = nln(SSR) + 2k $$ \n",
    "\n",
    "where $n$ is the number of observations. If we want to compare $N$ different models, we first normalize $AIC$ for each model as $\\Delta _{i}$. \n",
    "\n",
    "$$\\Delta _{i} = AIC_{i} - AIC_{min}$$ \n",
    "\n",
    "where $AIC_{min}$ is the best AIC.  The support for a given model is given by the Akaike weight ($w$):\n",
    "\n",
    "$$w_{i} = \\frac{exp(-\\Delta_{i}/2)}{\\sum_{j=0}^{j<N} exp(-\\Delta_{j}/2)}$$\n",
    "\n",
    "The weight runs from 0.0 to 1.0, with 1.0 representing the strongest support.  For a more thorough description, see [here](http://www.sortie-nd.org/lme/Statistical%20Papers/Burnham_and_Anderson_2004_Multimodel_Inference.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Create a function called **`calc_aic`** that returns the Akaike weight for a list of models.  **`calc_aic`** should take the following arguments:\n",
    "    + list of ssr for each fit\n",
    "    + list of num_parameters for each fit\n",
    "    + number of observations you fit the models to\n",
    "    \n",
    "#### You can check your function with the following lists.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these values\n",
    "ssr_list = [0.05,0.001,0.00095]\n",
    "num_parameters = [2,3,10]\n",
    "num_obs = 10\n",
    "\n",
    "# should give these weights\n",
    "weights = [8.69e-9,0.9988,1.18e-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real example: what can you learn from your data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sedimentation equilibrium experiments are used to study molecular assemblies.  If spin a solution of molecules in a centrifuge very fast for a very long time, you get an equlibirum distribution of molecules along the length ($r$) of the tube.  This is because centrigual force pulls them to the bottom of the tube, while diffusion tends to spread them out over the tube.  If you measure the concentration of molecules along the length of the tube $c(r)$, you can then fit a model and  back out the molecular weight(s) of the species in solution. \n",
    "\n",
    "The equation describing a simple, non-interacting molecule is below.  $c_{0}$ is the concentration of the molecule at $r=0$, $M$ is the molecular weight of the molecule, and $r$ is the position along the tube. \n",
    "\n",
    "$$c(r) = c_{0}exp \\Big [ M \\Big ( \\frac{r^{2}}{2} \\Big ) \\Big ]$$\n",
    "\n",
    "You are interested in whether there is any dimer present (a dimer is a pair of interacting molecules).  If there is dimer, our model gets more complicated.  We have to add a term $\\theta$ that describes the fraction of the molecules in dimer versus monomer:\n",
    "\n",
    "$$c(r) = c_{0} \\Big ( \\theta exp \\Big [ M \\Big ( \\frac{r^{2}}{2} \\Big ) \\Big ] + (1 - \\theta )exp \\Big [ 2M \\Big ( \\frac{r^{2}}{2} \\Big ) \\Big ] \\Big )$$\n",
    "\n",
    "\n",
    "(I've collapsed in a bunch of constants to simplify the problem. If you want a full discussion of the method, see [here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2711687/), particularly equation 4). \n",
    "\n",
    "#### Main question: do these data provide evidence for a dimer?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Implement a function and residuals function for each of these models. The first model should have the fittable parameters $c_{0}$ and $M$.  The second model should have the fittable parameters $c_{0}$, $M$, and $\\theta$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Fit both models to the data in `data/sed_eq.csv`.  What are your estimates of $c_{0}$, $M$, and $\\theta$?  Are they the same between the two fits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Use your `calc_aic` function on these fits. Which model is supported?  Can you conclude there is dimer present?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian\n",
    "\n",
    "One common type of data is a sum of Gaussian functions.  Among other places, these sort of data pop up in single-molecule work, fluorescence-activated cell sorting, spectroscopy and chromatography.  You collect a dataset that looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = pd.read_csv(\"data/gaussian.csv\")\n",
    "plt.plot(d.x,d.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You find code to analyze this kind of data on the internet.  Using the functions below, determine:\n",
    "\n",
    "+ how many gaussians you can extract from the data  in `lab-data/gaussian.csv`.\n",
    "+ their means, standard deviations, and areas. \n",
    "\n",
    "**PS: you should play with your initial guesses**\n",
    "**PPS: negative area gaussians are not allowed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_gaussian(x,means,stds,areas):\n",
    "    \"\"\"\n",
    "    Function calculating multiple gaussians (built from\n",
    "    values in means, stds, areas).  The number of gaussians\n",
    "    is determined by the length of means, stds, and areas.\n",
    "    The gaussian functions are calculated at values in\n",
    "    array x. \n",
    "    \"\"\"\n",
    "    \n",
    "    if len(means) != len(stds) or len(means) != len(areas):\n",
    "        err = \"means, standard deviations and areas should have the same length!\\n\"\n",
    "        raise ValueError(err)\n",
    "    \n",
    "    out = np.zeros(len(x),dtype=float)\n",
    "    for i in range(len(means)):\n",
    "        out += areas[i]*scipy.stats.norm(means[i],stds[i]).pdf(x)\n",
    "\n",
    "    return out\n",
    "\n",
    "def multi_gaussian_r(params,x,y):\n",
    "    \"\"\"\n",
    "    Residuals function for multi_guassian. \n",
    "    \"\"\"\n",
    "    \n",
    "    params = np.array(params)\n",
    "    if params.shape[0] % 3 != 0:\n",
    "        err = \"num parameters must be divisible by 3\\n\"\n",
    "        raise ValueError(err)\n",
    "    \n",
    "    means = params[np.arange(0,len(params),3)]\n",
    "    stds = params[np.arange(1,len(params),3)]\n",
    "    areas = params[np.arange(2,len(params),3)]\n",
    "    \n",
    "    return multi_gaussian(x,means,stds,areas) - y\n",
    "\n",
    "\n",
    "\n",
    "def fitter(x,y,means_guess,stds_guess,areas_guess):\n",
    "    \"\"\"\n",
    "    Fit an arbitrary number of gaussian functions to x/y data.\n",
    "    The number of gaussians that will be fit is determined by\n",
    "    the length of means_guess.  \n",
    "    \n",
    "    x: measurement x-values (array)\n",
    "    y: measurement y-values (array)\n",
    "    means_guess: array of guesses for means for gaussians.  \n",
    "                 length determines number of gaussians\n",
    "    stds_guess: array of guesses of standard deviations for\n",
    "                gaussians. length must match means_guess\n",
    "    areas_guess: array of area guesses for gaussians.  \n",
    "                 length must match means guess.\n",
    "                 \n",
    "    returns: means, stds, areas and fit sum-of-squared-residuals\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sanity check\n",
    "    if len(means_guess) != len(stds_guess) or len(means_guess) != len(areas_guess):\n",
    "        err = \"means, standard deviations and areas should have the same length!\\n\"\n",
    "        raise ValueError(err)\n",
    "    \n",
    "    # Construct an array of parameter guesses by assembling\n",
    "    # means, stds, and areas\n",
    "    param_guesses = []\n",
    "    for i in range(len(means_guess)):\n",
    "        param_guesses.append(means_guess[i])\n",
    "        param_guesses.append(stds_guess[i])\n",
    "        param_guesses.append(areas_guess[i])\n",
    "    param_guesses = np.array(param_guesses)\n",
    "    \n",
    "    # Fit the multigaussian function\n",
    "    fit = scipy.optimize.least_squares(multi_gaussian_r,param_guesses,\n",
    "                                       args=(x,y))\n",
    "    \n",
    "    # Disassemble into means, stds, areas\n",
    "    means = fit.x[np.arange(0,len(fit.x),3)]\n",
    "    stds = fit.x[np.arange(1,len(fit.x),3)]\n",
    "    areas = fit.x[np.arange(2,len(fit.x),3)]\n",
    "    \n",
    "    return means, stds, areas, fit.cost\n",
    "    \n",
    "def plot_gaussians(means,stds,areas):\n",
    "    \"\"\"\n",
    "    Plot a collection of gaussians.\n",
    "    \n",
    "    means: array of means for gaussians.  \n",
    "           length determines number of gaussians\n",
    "    stds: array of standard deviations for gaussians.\n",
    "          length must match means_guess\n",
    "    areas: array of areas for gaussians.  \n",
    "           length must match means guess.\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.plot(d.x,multi_gaussian(d.x,means,stds,areas))\n",
    "\n",
    "    for i in range(len(means)):\n",
    "        plt.plot(d.x,multi_gaussian(d.x,\n",
    "                                    [means[i]],\n",
    "                                    [stds[i]],\n",
    "                                    [areas[i]]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
